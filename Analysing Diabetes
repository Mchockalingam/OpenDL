
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

get_ipython().run_line_magic('matplotlib', 'inline')


# In[2]:


dataset = pd.read_csv('indian_liver_patient.csv')


# In[3]:


dataset.head()


# In[4]:


dataset.info()


# In[5]:


dataset.isnull().sum()


# # Content
# 
# This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh, India. The "Dataset" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.
# 
# Any patient whose age exceeded 89 is listed as being of age "90".
# 
# Columns:
# 
# Age of the patient
# 
# Gender of the patient
# 
# Total Bilirubin
# 
# Direct Bilirubin
# 
# Alkaline Phosphotase
# 
# Alamine Aminotransferase
# 
# Aspartate Aminotransferase
# 
# Total Protiens
# 
# Albumin
# 
# Albumin and Globulin Ratio
# 
# Dataset: field used to split the data into two sets (patient with liver disease, or no disease)
# 
# Acknowledgements
# 
# This dataset was downloaded from the UCI ML Repository:
# 
# Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.
# 
# Inspiration
# 
# Use these patient records to determine which patients have liver disease and which ones do not.

# In[6]:


sns.countplot(data = dataset, x = "Dataset", label = "Count")


# In[16]:


sns.countplot(data = dataset, x = "Gender", label = "Count", palette='viridis')


# We will see the relationship between the heads and how it impacts the dataset

# We will create a joint plot.

# In[28]:


sns.jointplot("Total_Bilirubin", "Direct_Bilirubin", data=dataset, kind="reg")


# In[14]:


correlations = dataset.corr()
plt.figure(figsize=(10,10))
g = sns.heatmap(correlations,cbar = True, square = True, annot=True, fmt= '.2f', annot_kws={'size': 10}, cmap='viridis')


# # Let us explore more using Machine Learning and determine which patients have liver disease and which ones do not.

# In[9]:


from sklearn import preprocessing
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn import decomposition
from sklearn.cluster import KMeans 
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score


# In[10]:


le = preprocessing.LabelEncoder()
le.fit(dataset['Gender'])


# In[11]:


dataset['Gender_Encoded'] = le.transform(dataset['Gender'])


# In[12]:


dataset.head()


# In[13]:


dataset['Albumin_and_Globulin_Ratio'].fillna(dataset['Albumin_and_Globulin_Ratio'].median(), inplace=True)


# We will grid for plotting pairwise relationships in a dataset.

# In[18]:


g = sns.PairGrid(dataset, hue = "Dataset", vars=['Age','Gender_Encoded','Total_Bilirubin','Total_Protiens'],palette='viridis')
g.map(plt.scatter)
plt.show()


# In[19]:


X = dataset[['Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',
        'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', 
        'Albumin', 'Albumin_and_Globulin_Ratio','Gender_Encoded']]
y = dataset[['Dataset']]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)
print(X_train.shape,X_test.shape)


# In[24]:


rf = RandomForestClassifier(n_estimators=25, random_state=0)
rf.fit(X_train, y_train)
rf_predicted = rf.predict(X_test)

random_forest_score      = round(rf.score(X_train, y_train) * 100, 2)
random_forest_score_test = round(rf.score(X_test, y_test) * 100, 2)

print('Random Forest Score: ', random_forest_score)
print('Random Forest Test Score: ', random_forest_score_test)
print('Accuracy: ', accuracy_score(y_test,rf_predicted))
print('\nClassification report: \n', classification_report(y_test,rf_predicted))

g = sns.heatmap(confusion_matrix(y_test,rf_predicted), annot=True, fmt="d",cmap='viridis')


# We have chosen Random Forest an ensemble learning since it produces higher accuracy in the traing set without overfitting the data.
# 
# Let us use Tensorflow and Keras optimisers

# In[25]:


import tensorflow as tf
from tensorflow import keras


# We will use relu and sigmoid activation function 

# In[26]:


from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32,input_shape=(10,),activation = 'relu'))
model.add(layers.Dense(8,activation = 'relu'))
model.add(layers.Dense(1,activation = 'sigmoid'))


# We will run the epoch 10 times on the training set.

# In[27]:


from keras import optimizers
model.compile(optimizer = optimizers.RMSprop(0.01),
             loss = 'binary_crossentropy',
             metrics = ['accuracy'])
model.fit(X_train,y_train,epochs = 10, batch_size = 100)


# # Let us explore some more thro logistic regression, Gaussian Naive Bayes using the data.

# In[44]:


# Convert Gender to 0s (for Male) and 1s (for Female)
dataset['Gender'] = dataset['Gender'].map({'Male':0, 'Female':1})
dataset['Dataset'] = dataset['Dataset'].map({2:0, 1:1}) 
# To solve ValueError: endog must be in the unit interval.

# Check if mapping happened properly
dataset


# In[45]:


# Correlations between variables help us identify the features that can be excluded. 
# We can exclude one in two features which has a strong correlatoin (|correlation| > 0.5) with another feature
# e.g: In the below given case, Total_Bilirubin and Direct_Bilirubin are strongly correlated. So we can
# discard one of those features. The exclusion of features happens 2 cells down.  
ip_corr = dataset.drop(['Gender', 'Dataset'], axis = 1)
ip_corr.corr()


# Split the dataset into train and test

# In[46]:


samples_count = dataset.shape[0]
# You will see drastic change in models' accuracy when you change the train and test sample proportion (80:20, 70:30 etc)
ip_train_count = int(0.8*samples_count) 
ip_test_count = samples_count - ip_train_count

ip_train_data = dataset[:ip_train_count]
ip_test_data = dataset[ip_train_count:]


# In[47]:


print(ip_train_count)
print(ip_test_count)


# Declare the Dependent (y...) and Independent (X...) variables

# In[48]:


import statsmodels.api as sm


# In[49]:


## You can start with all features and then optimize the model by removing the features from the model.

X1_train = ip_train_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',
              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]

#X1_train = ip_train_data[['Age', 'Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin']]

X_train= sm.add_constant(X1_train)
#X_train= X1_train.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant
y_train = ip_train_data['Dataset']


X1_test = ip_test_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',
              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]

#X1_test = ip_test_data[['Age','Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin' ]]

X_test = sm.add_constant(X1_test)
#X_test = X1_test.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant
y_test = ip_test_data['Dataset']


# # Logit function from statsmodels

# In[50]:


reg_log = sm.Logit(y_train.astype(float),X_train.astype(float))
result_log = reg_log.fit()


# In[51]:


result_log.summary2()


# In[52]:


result_log.predict()
result_log.pred_table()


# In[53]:


# Print X_test and X_train and see if the order of features are same in both datasets
X_test.head()


# In[54]:


X_train.head()


# In[55]:


logit_predicted = result_log.predict(X_test)


# In[57]:


#logit_predicted


# In[58]:


# Confusion Matrix of predicted values. if you wanted to use sklearn's 'confusion_matrix', 
# you should convert float values in 'logit_predicted' to 0s and 1s.

# Here I use a custom confusion matrix code 
bins = np.array([0,0.5,1])
cm_log = np.histogram2d(y_test, logit_predicted, bins = bins)[0]
logit_accuracy = (cm_log[0,0] + cm_log[1,1])/cm_log.sum()


# In[59]:


print('Confusion Matrix (Logit): \n', cm_log)
print('---------------------------------------------')
print('Accuracy (Logit): \n', round(logit_accuracy*100,2), '%')


# # Logitstic Regression (sklearn)

# In[60]:


from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
logreg.fit(X_train,y_train)

log_predicted = logreg.predict(X_test)


# In[61]:


print('Confusion Matrix (Logistic Reg): \n', confusion_matrix(y_test,log_predicted))
print('---------------------------------------------')
print('Accuracy (Logistict Reg): \n', round(accuracy_score(y_test, log_predicted)*100,2), '%')


# # Gaussian Naive Bayes

# In[62]:


from sklearn.naive_bayes import GaussianNB

gaussian = GaussianNB()
result_gauss = gaussian.fit(X_train,y_train)

gauss_predicted = gaussian.predict(X_test)


# In[63]:


print('Confusion Matrix (Gaussian): \n', confusion_matrix(y_test,gauss_predicted))
print('---------------------------------------------')
print('Accuracy (Gaussian): \n', round(accuracy_score(y_test, gauss_predicted)*100,2), '%')


# # Completed
